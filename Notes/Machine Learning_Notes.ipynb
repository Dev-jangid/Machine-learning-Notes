{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression  \n",
    "2. Logistic Regression  \n",
    "3. Decision Tree  \n",
    "4. SVM  \n",
    "5. Naive Bayes  \n",
    "6. kNN  \n",
    "7. K-Means  \n",
    "8. Random Forest  \n",
    "9. Dimensionality Reduction Algorithms  \n",
    "10. Gradient Boosting algorithms  \n",
    "    - GBM  \n",
    "    - XGBoost  \n",
    "    - LightGBM  \n",
    "    - CatBoost  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **1. Linear Regression**\n",
    "- **Definition:** A statistical method that models the relationship between a dependent variable and one or more independent variables using a linear equation.\n",
    "- **Equation:**  \n",
    "  \\[\n",
    "  Y = mX + b\n",
    "  \\]\n",
    "- **Assumptions:**\n",
    "  - Linearity\n",
    "  - Independence of errors\n",
    "  - Homoscedasticity (constant variance)\n",
    "  - Normal distribution of residuals\n",
    "- **Example:** Predicting house prices based on square footage.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- Be ready to explain **Mean Squared Error (MSE)**, **R-squared**, and how to handle **multicollinearity**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Logistic Regression**\n",
    "- **Definition:** A classification algorithm that estimates the probability of a binary outcome using the **sigmoid function**.\n",
    "- **Equation:**  \n",
    "  \\[\n",
    "  P(Y=1) = \\frac{1}{1 + e^{- (b + wX)}}\n",
    "  \\]\n",
    "- **Key Concept:** Outputs a probability; a threshold (e.g., 0.5) is used for classification.\n",
    "- **Example:** Email spam detection.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- Be ready to discuss **precision-recall tradeoff**, **ROC-AUC curve**, and why logistic regression is preferred over linear regression for classification.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Decision Tree**\n",
    "- **Definition:** A supervised learning algorithm that splits data into branches based on feature values.\n",
    "- **Concepts:** Uses **Gini impurity** or **Entropy** to determine the best split.\n",
    "- **Overfitting Solution:** Pruning, setting max depth.\n",
    "- **Example:** Loan approval prediction.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- Know the difference between **Entropy & Gini Impurity**.\n",
    "- Explain **overfitting** and how to prevent it.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Support Vector Machine (SVM)**\n",
    "- **Definition:** A classification algorithm that finds the optimal hyperplane that maximizes the margin between different classes.\n",
    "- **Key Concept:** Uses **Kernel Trick** (e.g., Polynomial, RBF) for non-linearly separable data.\n",
    "- **Example:** Image classification.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- Be ready to explain **Soft Margin vs. Hard Margin**.\n",
    "- Why SVMs work well for high-dimensional data.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Naive Bayes**\n",
    "- **Definition:** A probabilistic classifier based on **Bayes' Theorem**, assuming feature independence.\n",
    "- **Formula:**  \n",
    "  \\[\n",
    "  P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n",
    "  \\]\n",
    "- **Advantage:** Works well with large datasets and NLP tasks.\n",
    "- **Example:** Sentiment analysis.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- Why is it \"Naive\"? (Because of the independence assumption)\n",
    "- Explain **Laplace Smoothing**.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. k-Nearest Neighbors (kNN)**\n",
    "- **Definition:** A **non-parametric** algorithm that classifies based on the majority label of the k-nearest data points.\n",
    "- **Distance Metric:** **Euclidean, Manhattan, Minkowski**\n",
    "- **Example:** Handwritten digit recognition.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- Be ready to explain the **curse of dimensionality** and how it affects kNN.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. K-Means Clustering**\n",
    "- **Definition:** An unsupervised learning algorithm that partitions data into k clusters.\n",
    "- **Steps:**\n",
    "  - Choose k cluster centroids randomly.\n",
    "  - Assign data points to the nearest centroid.\n",
    "  - Update centroids iteratively.\n",
    "- **Example:** Customer segmentation.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- What is the **Elbow Method**? (Used to determine the optimal value of k)\n",
    "- Explain **inertia (sum of squared distances from centroids).**\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Random Forest**\n",
    "- **Definition:** An ensemble learning method that creates multiple decision trees and combines their outputs.\n",
    "- **Advantage:** Reduces **overfitting** compared to a single decision tree.\n",
    "- **Example:** Fraud detection.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- Difference between **Bagging & Boosting**.\n",
    "- Why does Random Forest handle missing values well?\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Dimensionality Reduction (PCA)**\n",
    "- **Definition:** Reduces the number of features while retaining important variance.\n",
    "- **How?** Converts correlated variables into uncorrelated **principal components**.\n",
    "- **Example:** Face recognition.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- Explain **Eigenvalues and Eigenvectors**.\n",
    "- What is **Explained Variance Ratio**?\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Gradient Boosting Machines (GBM)**\n",
    "- **Definition:** Boosting method that builds models sequentially, improving performance by correcting errors.\n",
    "- **Formula:**  \n",
    "  \\[\n",
    "  F_m(x) = F_{m-1}(x) + h_m(x)\n",
    "  \\]\n",
    "- **Example:** Loan default prediction.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- What is the **learning rate** and how does it affect performance?\n",
    "\n",
    "---\n",
    "\n",
    "## **11. XGBoost (Extreme Gradient Boosting)**\n",
    "- **Definition:** Optimized version of GBM with **L1 & L2 regularization**.\n",
    "- **Advantage:** Faster, better generalization.\n",
    "- **Example:** Credit scoring.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- Be ready to discuss **Tree Pruning** and **Early Stopping**.\n",
    "\n",
    "---\n",
    "\n",
    "## **12. LightGBM (Light Gradient Boosting Machine)**\n",
    "- **Definition:** A **leaf-wise growth algorithm** that is faster than XGBoost.\n",
    "- **Example:** Click-through rate (CTR) prediction.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- How does **Leaf-wise Growth** differ from **Level-wise Growth**?\n",
    "\n",
    "---\n",
    "\n",
    "## **13. CatBoost (Categorical Boosting)**\n",
    "- **Definition:** Gradient boosting optimized for **categorical data**.\n",
    "- **Advantage:** Handles categorical variables automatically.\n",
    "- **Example:** Product recommendations.\n",
    "\n",
    "**Interview Tip:**  \n",
    "- Why is CatBoost good for categorical features?\n",
    "\n",
    "---\n",
    "\n",
    "## **How to Answer in Interviews?**\n",
    "### **1. Structured Explanation:**\n",
    "> **\"Random Forest is an ensemble learning method that builds multiple decision trees and aggregates their results to improve accuracy and reduce overfitting. It uses bagging (bootstrap aggregating) to create diverse trees and combines their outputs via majority voting (classification) or averaging (regression). Compared to a single decision tree, Random Forest reduces variance and is more robust to noise.\"**\n",
    "\n",
    "### **2. Compare Algorithms:**\n",
    "> **\"The main difference between XGBoost and LightGBM is that XGBoost grows trees level-wise (depth-wise), while LightGBM grows trees leaf-wise. This makes LightGBM faster and more memory-efficient, especially for large datasets.\"**\n",
    "\n",
    "### **3. Application-Based Answer:**\n",
    "> **\"Logistic Regression is commonly used in healthcare for disease prediction. For example, it can predict whether a patient has diabetes based on features like blood sugar levels and BMI. The model outputs a probability, and a threshold (e.g., 0.5) is used to classify patients as diabetic or non-diabetic.\"**\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Interview Tips**\n",
    "âœ… **Explain Key Concepts** (Donâ€™t just define; provide insights).  \n",
    "âœ… **Know Assumptions & Limitations** (Every model has constraints).  \n",
    "âœ… **Mention Real-World Applications** (How is it used in industry?).  \n",
    "âœ… **Prepare Trade-offs & Comparisons** (E.g., \"When to use SVM vs. Random Forest?\").  \n",
    "âœ… **Be Ready for Follow-Up Questions** (e.g., \"How do you tune hyperparameters?\").  \n",
    "\n",
    "Would you like **sample code** for any of these? ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
