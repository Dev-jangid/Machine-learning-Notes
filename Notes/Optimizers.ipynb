{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2798cbb3",
   "metadata": {},
   "source": [
    "Optimizers are crucial in training machine learning models as they adjust the model's parameters to minimize the loss function. Here’s an overview of some commonly used optimizers and their characteristics:\n",
    "\n",
    "---\n",
    "\n",
    "### **Popular Optimizers**\n",
    "1. **Stochastic Gradient Descent (SGD)**:\n",
    "   - **Description**: Updates weights using a small random batch of the dataset, reducing computational cost compared to full-batch gradient descent.\n",
    "   - **Strengths**: Simple and effective for many tasks.\n",
    "   - **Weaknesses**: Can converge slowly and might get stuck in local minima.\n",
    "\n",
    "2. **Momentum**:\n",
    "   - **Description**: Enhances SGD by adding a momentum term to smooth the gradient updates and accelerate convergence.\n",
    "   - **Strengths**: Helps escape local minima and speeds up training.\n",
    "   - **Weaknesses**: Can overshoot the optimal solution without careful tuning.\n",
    "\n",
    "3. **Adam (Adaptive Moment Estimation)**:\n",
    "   - **Description**: Combines the benefits of RMSProp and Momentum by using adaptive learning rates and momentum.\n",
    "   - **Strengths**: Performs well out of the box and handles sparse gradients effectively.\n",
    "   - **Weaknesses**: May generalize poorly in some cases if hyperparameters aren’t tuned.\n",
    "\n",
    "4. **AdamW (Adam with Weight Decay)**:\n",
    "   - **Description**: A variant of Adam that decouples weight decay from the gradient update, improving generalization.\n",
    "   - **Strengths**: Regularizes better, especially for modern deep networks.\n",
    "   - **Weaknesses**: Requires slightly more hyperparameter tuning.\n",
    "\n",
    "5. **Adagrad**:\n",
    "   - **Description**: Adapts the learning rate based on past gradient information, using smaller updates for frequently updated parameters.\n",
    "   - **Strengths**: Suitable for sparse datasets like NLP.\n",
    "   - **Weaknesses**: Learning rate diminishes too quickly, which can slow down convergence.\n",
    "\n",
    "6. **RMSProp**:\n",
    "   - **Description**: Modifies Adagrad by using a moving average of squared gradients to maintain a consistent learning rate.\n",
    "   - **Strengths**: Works well for RNNs and non-convex problems.\n",
    "   - **Weaknesses**: May require careful tuning for specific tasks.\n",
    "\n",
    "7. **Nadam (Nesterov-accelerated Adam)**:\n",
    "   - **Description**: Combines Adam with Nesterov momentum for faster convergence.\n",
    "   - **Strengths**: Often achieves better performance in practice.\n",
    "   - **Weaknesses**: Slightly more complex to understand and implement.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Optimizers Work**\n",
    "All optimizers aim to minimize the loss function by updating model parameters based on gradients. The choice of optimizer depends on:\n",
    "- **Task** (e.g., image classification, text generation).\n",
    "- **Model Complexity** (e.g., deep networks with many layers may benefit from AdamW).\n",
    "- **Dataset** (e.g., sparse datasets may need Adagrad or Adam).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a deeper dive into a specific optimizer, its implementation, or practical use cases? Let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98efb171",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918fa12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be40900d",
   "metadata": {},
   "source": [
    "Optimizers play a central role in training machine learning models. The choice of optimizer depends on **where** it's applied (the type of task or model) and **why** it's suited for that specific purpose. Here's a breakdown of common scenarios and optimizers used:\n",
    "\n",
    "---\n",
    "\n",
    "### **Where We Use Optimizers**\n",
    "1. **Computer Vision**:\n",
    "   - Tasks: Image classification, object detection, segmentation.\n",
    "   - Models: Convolutional Neural Networks (CNNs), ResNet, Faster RCNN.\n",
    "   - Optimizers:\n",
    "     - **Adam**: Handles large datasets efficiently and converges quickly.\n",
    "     - **SGD with Momentum**: Often preferred for tasks requiring high precision (e.g., image classification).\n",
    "     - **AdamW**: Used in transformer-based models like ViT for better regularization.\n",
    "\n",
    "2. **Natural Language Processing (NLP)**:\n",
    "   - Tasks: Text generation, sentiment analysis, machine translation.\n",
    "   - Models: LSTMs, RNNs, transformer models (e.g., BERT, GPT).\n",
    "   - Optimizers:\n",
    "     - **Adam**: Works well for sparse gradients, common in NLP.\n",
    "     - **RMSProp**: Effective for sequence models due to its adaptive learning rate.\n",
    "     - **AdamW**: Commonly used for large transformer-based architectures.\n",
    "\n",
    "3. **Reinforcement Learning**:\n",
    "   - Tasks: Game playing, robotics, simulation-based tasks.\n",
    "   - Models: Policy networks, Q-learning networks.\n",
    "   - Optimizers:\n",
    "     - **RMSProp**: Manages non-convex loss surfaces typical in reinforcement learning.\n",
    "     - **Adam**: Performs well with unstable gradient updates.\n",
    "\n",
    "4. **Time-Series Analysis**:\n",
    "   - Tasks: Forecasting, anomaly detection.\n",
    "   - Models: Recurrent Neural Networks (RNNs), LSTMs, GRUs.\n",
    "   - Optimizers:\n",
    "     - **RMSProp**: Particularly useful for recurrent architectures.\n",
    "     - **Adam**: A general-purpose optimizer for handling sequential data.\n",
    "\n",
    "5. **Generative Models**:\n",
    "   - Tasks: GANs, autoencoders, image synthesis.\n",
    "   - Models: Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs).\n",
    "   - Optimizers:\n",
    "     - **SGD**: Commonly used in GANs to ensure stability.\n",
    "     - **Adam**: Preferred for faster convergence in generative tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why We Use Specific Optimizers**\n",
    "1. **Adam**:\n",
    "   - **Why**: Combines momentum and adaptive learning rates, making it versatile and effective across tasks. Works out of the box with default settings.\n",
    "   - **Where**: NLP, vision, generative tasks.\n",
    "\n",
    "2. **SGD with Momentum**:\n",
    "   - **Why**: Provides stable convergence and precision, especially in deep networks. Momentum helps avoid local minima.\n",
    "   - **Where**: Computer vision tasks like ResNet training.\n",
    "\n",
    "3. **RMSProp**:\n",
    "   - **Why**: Adapts learning rates based on the magnitude of gradients, ensuring stable updates for tasks like reinforcement learning and time-series.\n",
    "   - **Where**: Sequential models and reinforcement learning.\n",
    "\n",
    "4. **AdamW**:\n",
    "   - **Why**: Improves generalization by decoupling weight decay from gradient updates. Ideal for transformer-based architectures.\n",
    "   - **Where**: Vision transformers, large-scale NLP models.\n",
    "\n",
    "5. **Adagrad**:\n",
    "   - **Why**: Adapts learning rates for each parameter, making it useful for sparse datasets (e.g., NLP tasks).\n",
    "   - **Where**: Sparse gradient scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "Each optimizer has strengths and weaknesses, and selecting the right one depends on your model architecture, dataset characteristics, and the task's requirements. Would you like to see examples of how to implement these optimizers in code? Or perhaps explore their impact on specific tasks like deep neural networks or transformers? Let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1af456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3de3c22",
   "metadata": {},
   "source": [
    "Let's explore a detailed comparison of **optimizers** with their underlying mechanisms, strengths, weaknesses, and practical applications. I'll focus on some of the most widely-used ones: **SGD**, **Adam**, **RMSProp**, and **AdamW**. Here's the breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Mechanisms of Optimizers**\n",
    "\n",
    "| Optimizer  | **Update Rule**                                                                                      | **Key Feature**                                      | **Learning Rate**                  |\n",
    "|------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------|-------------------------------------|\n",
    "| **SGD**    | $$\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)$$                                                | Simple gradient descent                             | Fixed                              |\n",
    "| **RMSProp**| $$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2] + \\epsilon}} \\cdot \\nabla L(\\theta_t)$$          | Uses moving average of squared gradients            | Adaptive                           |\n",
    "| **Adam**   | $$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon}$$                    | Combines momentum and adaptive learning rates       | Adaptive                           |\n",
    "| **AdamW**  | $$\\theta_{t+1} = \\theta_t - \\eta \\cdot (\\text{gradient} + \\lambda \\cdot \\theta)$$ (weight decay decoupled) | Better regularization via decoupled weight decay    | Adaptive                           |\n",
    "\n",
    "Where:\n",
    "- $$\\theta$$: Model parameters\n",
    "- $$\\eta$$: Learning rate\n",
    "- $$\\nabla L$$: Gradient of the loss function\n",
    "- $$E[g^2]$$: Moving average of squared gradients\n",
    "- $$\\hat{m}, \\hat{v}$$: Momentum terms for first and second moments\n",
    "- $$\\lambda$$: Weight decay coefficient (AdamW)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Strengths and Weaknesses**\n",
    "\n",
    "| Optimizer  | **Strengths**                                                                 | **Weaknesses**                                           |\n",
    "|------------|------------------------------------------------------------------------------|----------------------------------------------------------|\n",
    "| **SGD**    | Simple and efficient; converges well for convex problems                    | Slow convergence; struggles with non-convex surfaces     |\n",
    "| **RMSProp**| Works well for recurrent networks (e.g., LSTMs)                              | Requires careful tuning of learning rate                 |\n",
    "| **Adam**   | Performs well out of the box; good for sparse gradients                      | May overfit and generalize poorly in some cases          |\n",
    "| **AdamW**  | Improves generalization; better for transformer architectures               | Slightly more complex to tune due to weight decay        |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Performance Comparison**\n",
    "\n",
    "| Optimizer  | **Speed of Convergence** | **Generalization** | **Best for**                     |\n",
    "|------------|--------------------------|--------------------|-----------------------------------|\n",
    "| **SGD**    | Moderate                 | Good               | Standard CNNs (e.g., ResNet)      |\n",
    "| **RMSProp**| Faster for RNNs          | Moderate           | Sequential tasks (e.g., LSTMs)    |\n",
    "| **Adam**   | Fast                     | Moderate           | Sparse gradients, NLP             |\n",
    "| **AdamW**  | Fast                     | Excellent          | Large-scale models (e.g., BERT)   |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Use Cases**\n",
    "\n",
    "- **SGD**: Ideal for computer vision tasks like **image classification**, where precision is critical, and training datasets are huge.\n",
    "- **RMSProp**: Works well in **time-series tasks** and recurrent architectures, such as **forecasting** and **anomaly detection**.\n",
    "- **Adam**: Dominant in **NLP** and **reinforcement learning**, especially for tasks involving sparse gradients (e.g., transformer models).\n",
    "- **AdamW**: Preferred for modern architectures like **Vision Transformers (ViT)** and **BERT** due to its enhanced regularization.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table:\n",
    "\n",
    "| Optimizer  | **Mechanism**                     | **Strength**                           | **Weakness**                | **Ideal Task**              |\n",
    "|------------|-----------------------------------|----------------------------------------|-----------------------------|-----------------------------|\n",
    "| **SGD**    | Basic gradient updates           | Simple; efficient for large datasets   | Slow on non-convex problems | Computer vision             |\n",
    "| **RMSProp**| Adaptive learning rate           | Handles sequential data well           | Learning rate tuning        | Time-series analysis        |\n",
    "| **Adam**   | Combines momentum + RMSProp      | Fast convergence; sparse gradients     | May overfit                 | NLP, sparse gradients       |\n",
    "| **AdamW**  | Decouples weight decay           | Better regularization; fast convergence| Complex tuning              | Transformers (BERT, ViT)    |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to see code snippets implementing these optimizers, or dive deeper into one of them? Let me know how you'd like to proceed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e25f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41dddcfc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "301a75b5",
   "metadata": {},
   "source": [
    "When it comes to tasks like **large language models (LLMs)**, computer vision for **videos and images**, **text**, and **speech processing**, different optimizers are suitable for different domains due to their unique data characteristics and learning requirements. Here's a detailed guide:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Large Language Models (LLMs)**:\n",
    "   - **Examples**: GPT, BERT, LLaMA.\n",
    "   - **Challenges**: LLMs involve massive parameter spaces and sparse gradients, requiring stability and scalability in optimization.\n",
    "   - **Preferred Optimizers**:\n",
    "     - **AdamW**: Commonly used for LLMs due to its decoupled weight decay, which prevents overfitting and ensures better generalization. Works well with transformers.\n",
    "     - **Adafactor**: A memory-efficient variant of Adam, often used in large-scale models like T5 to reduce memory overhead.\n",
    "   - **Why**: Adaptive learning rates (Adam-like optimizers) handle sparse gradients and ensure convergence despite the immense number of parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Computer Vision (Images and Videos)**:\n",
    "   - **Tasks**: Object detection, segmentation, classification, video analysis.\n",
    "   - **Challenges**: High-dimensional image data, varying object scales, real-time processing for video.\n",
    "   - **Preferred Optimizers**:\n",
    "     - **SGD with Momentum**: Works well for convolutional networks (e.g., ResNet, Faster RCNN), offering precision and stable convergence.\n",
    "     - **Adam**: Suitable for tasks requiring faster convergence (e.g., training CNNs on small or medium-scale datasets).\n",
    "     - **AdamW**: Frequently used for vision transformer models (ViT) due to its ability to handle large architectures effectively.\n",
    "   - **Why**: SGD dominates in vision tasks where overfitting needs to be minimized, while Adam/AdamW is great for transformer-based models or speed-critical applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Text Processing**:\n",
    "   - **Examples**: Sentiment analysis, summarization, translation.\n",
    "   - **Challenges**: Sequential dependencies, sparse gradients in NLP tasks.\n",
    "   - **Preferred Optimizers**:\n",
    "     - **Adam**: Performs well for sparse gradient updates in transformers like BERT and GPT.\n",
    "     - **RMSProp**: Useful for RNNs or LSTMs dealing with sequential dependencies.\n",
    "     - **AdamW**: Helps modern NLP models generalize better, especially on large datasets.\n",
    "   - **Why**: Adaptive learning optimizers like Adam ensure faster convergence and manage sparse gradients effectively, crucial for text-based tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Speech Processing**:\n",
    "   - **Examples**: Speech-to-text, text-to-speech, voice recognition.\n",
    "   - **Challenges**: Temporal dependencies, large audio datasets, handling high variance in speech data (e.g., accents, noise).\n",
    "   - **Preferred Optimizers**:\n",
    "     - **RMSProp**: Performs well with sequential models like RNNs, LSTMs, and GRUs, which are common in speech tasks.\n",
    "     - **Adam**: Used for transformer-based models like Wav2Vec or audio encoders due to its robust performance.\n",
    "     - **SGD with Momentum**: Effective for smaller, more specialized speech models requiring fine-grained updates.\n",
    "   - **Why**: Speech tasks demand optimizers that handle long-term dependencies and noisy data effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table: Optimizers Across Domains\n",
    "\n",
    "| **Domain**            | **Tasks**                 | **Optimizers**                 | **Why It Works**                                               |\n",
    "|------------------------|---------------------------|---------------------------------|---------------------------------------------------------------|\n",
    "| **LLMs**              | Language generation, Q&A | AdamW, Adafactor               | Handles sparse gradients and large parameter spaces            |\n",
    "| **Computer Vision**    | Classification, detection| SGD + Momentum, Adam, AdamW    | Precision, regularization, and stability for deep architectures|\n",
    "| **Text Processing**    | Sentiment, translation   | Adam, AdamW, RMSProp           | Effective for sparse gradients and sequential data             |\n",
    "| **Speech Processing**  | Speech-to-text, TTS      | RMSProp, Adam, SGD             | Manages temporal dependencies and high data variance           |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to dive deeper into one of these domains, their challenges, or implementation strategies for these optimizers in tasks like training transformers, CNNs, or RNNs? Let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8416f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c67def5b",
   "metadata": {},
   "source": [
    "Optimizers tailored for tasks across domains like LLMs, computer vision, text processing, and speech recognition. Here's how their challenges, implementations, and optimizer choices come together:\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizers in Large Language Models (LLMs)**\n",
    "\n",
    "#### **Challenges**:\n",
    "- Huge parameter space (hundreds of millions to billions).\n",
    "- Sparse gradients due to attention mechanisms in transformer architectures.\n",
    "\n",
    "#### **Preferred Optimizers**:\n",
    "1. **AdamW**:\n",
    "   - Used in BERT, GPT, and other transformer-based models.\n",
    "   - **Why**: Decouples weight decay from gradient updates, ensuring better regularization for large-scale architectures.\n",
    "\n",
    "2. **Adafactor**:\n",
    "   - Memory-efficient alternative to Adam, popular in T5 models.\n",
    "   - **Why**: Reduces memory consumption during optimization without sacrificing performance.\n",
    "\n",
    "#### **Implementation**:\n",
    "```python\n",
    "from transformers import AdamW\n",
    "\n",
    "# Example: Optimizing a transformer model\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizers in Computer Vision (Images and Videos)**\n",
    "\n",
    "#### **Challenges**:\n",
    "- High-dimensional data for images/videos.\n",
    "- Varying object scales, cluttered backgrounds, real-time processing.\n",
    "\n",
    "#### **Preferred Optimizers**:\n",
    "1. **SGD with Momentum**:\n",
    "   - Ideal for convolutional networks like ResNet, Faster RCNN.\n",
    "   - **Why**: Offers precise and stable convergence, critical for large vision datasets.\n",
    "\n",
    "2. **Adam**:\n",
    "   - For smaller datasets or faster convergence needs.\n",
    "   - **Why**: Adaptive learning rates simplify training of models like MobileNet.\n",
    "\n",
    "3. **AdamW**:\n",
    "   - Used in Vision Transformers (ViT).\n",
    "   - **Why**: Works well with transformer architectures for large-scale image classification.\n",
    "\n",
    "#### **Implementation**:\n",
    "```python\n",
    "from torch.optim import SGD\n",
    "\n",
    "# Example: Optimizing a CNN model\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Example: Optimizing Vision Transformer (ViT) with AdamW\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizers in Text Processing**\n",
    "\n",
    "#### **Challenges**:\n",
    "- Sequential dependencies in data.\n",
    "- Sparse gradients in transformer-based architectures.\n",
    "\n",
    "#### **Preferred Optimizers**:\n",
    "1. **Adam**:\n",
    "   - Default choice for NLP transformers like BERT and GPT.\n",
    "   - **Why**: Handles sparse gradients effectively.\n",
    "\n",
    "2. **RMSProp**:\n",
    "   - Used for RNNs and LSTMs.\n",
    "   - **Why**: Adaptive learning rate simplifies training sequential models.\n",
    "\n",
    "3. **AdamW**:\n",
    "   - Regularizes transformer-based models better than Adam.\n",
    "\n",
    "#### **Implementation**:\n",
    "```python\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "# Example: Optimizing an RNN model\n",
    "optimizer = RMSprop(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Example: Optimizing BERT with AdamW\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizers in Speech Processing**\n",
    "\n",
    "#### **Challenges**:\n",
    "- Temporal dependencies in sequential data.\n",
    "- High variance due to accents, noise, and pitch changes.\n",
    "\n",
    "#### **Preferred Optimizers**:\n",
    "1. **RMSProp**:\n",
    "   - Effective for recurrent architectures like RNNs and LSTMs.\n",
    "   - **Why**: Smooths learning rates across gradient updates, handling variability in temporal sequences.\n",
    "\n",
    "2. **Adam**:\n",
    "   - Used in transformer-based audio models like Wav2Vec.\n",
    "   - **Why**: Optimizes performance while managing sparse gradient updates.\n",
    "\n",
    "#### **Implementation**:\n",
    "```python\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "# Example: Optimizing an LSTM for speech data\n",
    "optimizer = RMSprop(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Example: Optimizing Wav2Vec with Adam\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Table: Optimizers Across Domains\n",
    "\n",
    "| **Domain**            | **Models**                  | **Optimizers**                 | **Why It Works**                                               |\n",
    "|------------------------|-----------------------------|---------------------------------|---------------------------------------------------------------|\n",
    "| **LLMs**              | BERT, GPT, LLaMA           | AdamW, Adafactor               | Regularization and sparse gradients for large parameters       |\n",
    "| **Computer Vision**    | ResNet, ViT, Faster RCNN   | SGD + Momentum, Adam, AdamW    | Precision and stability for CNNs; generalization for ViTs      |\n",
    "| **Text Processing**    | RNNs, BERT, GPT           | Adam, RMSProp, AdamW           | Sparse gradients and sequence modeling                        |\n",
    "| **Speech Processing**  | LSTMs, Wav2Vec            | RMSProp, Adam                  | Temporal dependencies and managing high variance              |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to focus on implementing a specific model or explore how these optimizers affect the performance of real-world tasks like training transformers, CNNs, or RNNs? Let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba1851",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
